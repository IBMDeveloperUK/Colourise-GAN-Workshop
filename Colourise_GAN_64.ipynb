{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Generative Adversarial Networks (GANs)\n",
    "\n",
    "In this workshop we will be learning what a GAN is and where they are used. We will be building and training a simple GAN to\n",
    "colour black and white images.\n",
    "\n",
    "For this workshop we will be using IBM Watson Studio, an interactive online Python notebook.\n",
    "\n",
    "The machine learning library used for this is Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you are running on a CPU instance (e.g. a default lite/free plan of Watson Studio) \n",
    "### then pyTorch will not be installed by default and we need to install it, note this may\n",
    "### take a few minutes to complete\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    !conda install pytorch-cpu -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "b25e70a71a5d48548f60549b8dff2d10",
      "38c8da1b2c324c5a9658abb0bbf90ce6",
      "c349c2d356044831a60daba865323c81",
      "5ea5f5c8899741708b26626113671794",
      "cd0b16c4f188467e8038ad89a34c6568",
      "f4c1628e9cb241e1885131fa4e5e5cb1",
      "bf239b6d31a64b0fa47ec6f3c32072e6",
      "9ae6f1232207473eb6ae3ae4d775d766",
      "4634059c11064b448927b68a92e24462",
      "c15492a30b7c4deabfcc63d47bbc4d77",
      "8aeb772ded0a42748aa70a5284ba6801",
      "d4c658434d884309b274e9759a5d85b8",
      "7ee99db161c240a18670015c7d59d464",
      "6bcb28cf89ef4c9883619ceddbd3e061",
      "c2741226cc40492eaabccaf8775a0654",
      "15973d3fd2904fbb996cf841302a9099"
     ]
    },
    "colab_type": "code",
    "id": "BQt_TFUkmG0D",
    "outputId": "2bd637ec-bf5d-4f43-e5dd-08bdc66349cc"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, math\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import requests\n",
    "import warnings\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2yuv, yuv2rgb\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device\", device)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Data\n",
    "\n",
    "We need to be able to load in data for training. There is a zip file with a selection of images taken from the 'CelebA' \n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "b25e70a71a5d48548f60549b8dff2d10",
      "38c8da1b2c324c5a9658abb0bbf90ce6",
      "c349c2d356044831a60daba865323c81",
      "5ea5f5c8899741708b26626113671794",
      "cd0b16c4f188467e8038ad89a34c6568",
      "f4c1628e9cb241e1885131fa4e5e5cb1",
      "bf239b6d31a64b0fa47ec6f3c32072e6",
      "9ae6f1232207473eb6ae3ae4d775d766",
      "4634059c11064b448927b68a92e24462",
      "c15492a30b7c4deabfcc63d47bbc4d77",
      "8aeb772ded0a42748aa70a5284ba6801",
      "d4c658434d884309b274e9759a5d85b8",
      "7ee99db161c240a18670015c7d59d464",
      "6bcb28cf89ef4c9883619ceddbd3e061",
      "c2741226cc40492eaabccaf8775a0654",
      "15973d3fd2904fbb996cf841302a9099"
     ]
    },
    "colab_type": "code",
    "id": "BQt_TFUkmG0D",
    "outputId": "2bd637ec-bf5d-4f43-e5dd-08bdc66349cc"
   },
   "outputs": [],
   "source": [
    "FACES_URL = \"https://github.com/IBMDeveloperUK/Colourise-GAN-Workshop/raw/master/data/faces64_1000.zip\"\n",
    "# If you have a GPU and want more data to train on, try the larger dataset below\n",
    "#FACES_URL = \"https://github.com/IBMDeveloperUK/Colourise-GAN-Workshop/raw/master/data/faces64_10000.zip\"\n",
    "\n",
    "# These values affect the speed and rate of learning\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "pixel_loss_weights = 100.0\n",
    "\n",
    "# Number of epochs to run. On a CPU instance it will take a few minutes per epoch\n",
    "# if you are running on a GPU try increasing this to 10-30\n",
    "n_epochs = 1\n",
    "\n",
    "# A number of torch dataset iterators that can download images and get them\n",
    "# in the right format for what we need.\n",
    "class rgb_img_data(Dataset):\n",
    "    def __init__(self, url):\n",
    "        r = requests.get(url)\n",
    "        self.z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        self.files = self.z.namelist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.files[index]\n",
    "        img = Image.open(self.z.open(filename))\n",
    "        if img.mode != 'RGB':\n",
    "          img = img.convert('RGB')\n",
    "        return img\n",
    "\n",
    "class lab_img_data(Dataset):\n",
    "    def __init__(self, rgb_img_gen):\n",
    "        self.rgb_img_gen = rgb_img_gen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_img_gen)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.rgb_img_gen[index]\n",
    "        lab = rgb2lab(img)\n",
    "        return lab\n",
    "    \n",
    "class tensor_img_data(Dataset):\n",
    "    def __init__(self, lab_img_gen):\n",
    "        self.lab_img_gen = lab_img_gen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lab_img_gen)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lab = self.lab_img_gen[index]\n",
    "        l = lab[...,0] / 50 - 1\n",
    "        a_t = lab[...,1] / 128\n",
    "        b_t = lab[...,2] / 128\n",
    "        return torch.Tensor(np.expand_dims(l,axis=0)),torch.Tensor(np.stack([a_t,b_t],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "\n",
    "# Get the iterator for the source RGB images\n",
    "rgb_images = rgb_img_data(FACES_URL)\n",
    "fig = plt.figure(figsize=[12,4])\n",
    "fig.suptitle('Original RGB images') \n",
    "for i in range(5):\n",
    "    plt.subplot(2, n_samples, 1 + i)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    rgb_img = np.asarray(rgb_images[i])\n",
    "\n",
    "    plt.imshow(rgb_img)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# create an iterator for LAB-format images so we can display the greyscale versions\n",
    "lab_images = lab_img_data(rgb_images)\n",
    "fig = plt.figure(figsize=[12,4])\n",
    "fig.suptitle('Greyscale images') \n",
    "for i in range(n_samples):\n",
    "    plt.subplot(2, n_samples, 1 + i)\n",
    "    plt.axis('off')\n",
    "\n",
    "    yuv_img = np.asarray(lab_images[i])\n",
    "\n",
    "    plt.imshow(yuv_img[:,:,0], cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Models\n",
    "\n",
    "We need to define a model for both the \"generator\" and the \"discriminator\". We define a GeneratorEncoderBlock and GeneratorDecoderBlock as we re-use the blocks multiple times in the networks.\n",
    "\n",
    "The generator network is a U-Net, which means that it has a \"contraction\" followed by an \"expansion\" phase (like an autoencoder) but that it has a number of skip connections to join the corresponding blocks on each side of the U. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "b25e70a71a5d48548f60549b8dff2d10",
      "38c8da1b2c324c5a9658abb0bbf90ce6",
      "c349c2d356044831a60daba865323c81",
      "5ea5f5c8899741708b26626113671794",
      "cd0b16c4f188467e8038ad89a34c6568",
      "f4c1628e9cb241e1885131fa4e5e5cb1",
      "bf239b6d31a64b0fa47ec6f3c32072e6",
      "9ae6f1232207473eb6ae3ae4d775d766",
      "4634059c11064b448927b68a92e24462",
      "c15492a30b7c4deabfcc63d47bbc4d77",
      "8aeb772ded0a42748aa70a5284ba6801",
      "d4c658434d884309b274e9759a5d85b8",
      "7ee99db161c240a18670015c7d59d464",
      "6bcb28cf89ef4c9883619ceddbd3e061",
      "c2741226cc40492eaabccaf8775a0654",
      "15973d3fd2904fbb996cf841302a9099"
     ]
    },
    "colab_type": "code",
    "id": "BQt_TFUkmG0D",
    "outputId": "2bd637ec-bf5d-4f43-e5dd-08bdc66349cc"
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution\n",
    "def get_same_padding(x, kernel_sz, stride, dialation):\n",
    "    return max((math.ceil(x / stride) - 1) * stride + (kernel_sz - 1) * dialation + 1 - x, 0)\n",
    "\n",
    "# Dynamically pad input x with 'SAME' padding for conv with specified args\n",
    "def pad_same(x, kernel_sz, stride, dialation=(1, 1), value=0.):\n",
    "    ih, iw = x.size()[-2:]\n",
    "    pad_h, pad_w = get_same_padding(ih, kernel_sz[0], stride[0], dialation[0]), get_same_padding(iw, kernel_sz[1],\n",
    "                                                                                                 stride[1],\n",
    "                                                                                                 dialation[1])\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)\n",
    "    return x\n",
    "\n",
    "class GeneratorEncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, batch_norm=True, kernel_size=4):\n",
    "        super(GeneratorEncoderBlock, self).__init__()\n",
    "\n",
    "        layer_modules = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        ]\n",
    "        if batch_norm == True:\n",
    "            layer_modules.append(nn.BatchNorm2d(out_channels))\n",
    "        layer_modules.append(nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.encoder = nn.Sequential(*layer_modules)\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = pad_same(x, kernel_sz=(self.kernel_size, self.kernel_size), stride=(self.stride, self.stride))\n",
    "        return self.encoder(x)\n",
    "\n",
    "class GeneratorDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, batch_norm=True, kernel_size=4, padding= 1):\n",
    "        super(GeneratorDecoderBlock, self).__init__()\n",
    "        layer_modules = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        ]\n",
    "        if batch_norm == True:\n",
    "            layer_modules.append(nn.BatchNorm2d(out_channels))\n",
    "        layer_modules.append(nn.ReLU())\n",
    "        self.decoder = nn.Sequential(*layer_modules)\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, name, output_channels=2, training=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.name = name\n",
    "        self.output_channels = output_channels\n",
    "        self.training = training\n",
    "        self.encoder_block_1 = GeneratorEncoderBlock(1, 64, 1)\n",
    "        self.encoder_block_2 = GeneratorEncoderBlock(64, 64, 2)\n",
    "        self.encoder_block_3 = GeneratorEncoderBlock(64, 128, 2)\n",
    "        self.encoder_block_4 = GeneratorEncoderBlock(128, 256, 2)\n",
    "        self.encoder_block_5 = GeneratorEncoderBlock(256, 512, 2)\n",
    "        self.encoder_block_6 = GeneratorEncoderBlock(512, 512, 2)\n",
    "\n",
    "        self.decoder_block_1 = GeneratorDecoderBlock(512, 512, 2)\n",
    "        self.decoder_block_2 = GeneratorDecoderBlock(512, 256, 2)\n",
    "        self.decoder_block_3 = GeneratorDecoderBlock(256, 128, 2)\n",
    "        self.decoder_block_4 = GeneratorDecoderBlock(128, 64, 2)\n",
    "        self.decoder_block_5 = GeneratorDecoderBlock(64, 64, 2)\n",
    "        self.last_layer = nn.Conv2d(64, self.output_channels, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        layers = []\n",
    "        output = self.encoder_block_1(inputs)\n",
    "        layers.append(output)\n",
    "        output = self.encoder_block_2(output)\n",
    "        layers.append(output)\n",
    "        output = self.encoder_block_3(output)\n",
    "        layers.append(output)\n",
    "        output = self.encoder_block_4(output)\n",
    "        layers.append(output)\n",
    "        output = self.encoder_block_5(output)\n",
    "        layers.append(output)\n",
    "        output = self.encoder_block_6(output)\n",
    "        layers.append(output)\n",
    "\n",
    "        output = self.decoder_block_1(output)\n",
    "        output = output + layers[len(layers) - 0 - 2]\n",
    "\n",
    "        output = self.decoder_block_2(output)\n",
    "        output = output + layers[len(layers) - 1 - 2]\n",
    "\n",
    "        output = self.decoder_block_3(output)\n",
    "        output = output + layers[len(layers) - 2 - 2]\n",
    "\n",
    "        output = self.decoder_block_4(output)\n",
    "        output = output + layers[len(layers) - 3 - 2]\n",
    "\n",
    "        output = self.decoder_block_5(output)\n",
    "        output = output + layers[len(layers) - 4 - 2]\n",
    "\n",
    "        output = torch.tanh(self.last_layer(output))\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, name, training=True):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.name = name\n",
    "        self.training = training\n",
    "        self.discriminator_block_1 = GeneratorEncoderBlock(3, 64, 2, False)\n",
    "        self.discriminator_block_2 = GeneratorEncoderBlock(64, 128, 2)\n",
    "        self.discriminator_block_3 = GeneratorEncoderBlock(128, 256, 2)\n",
    "        self.discriminator_block_4 = GeneratorEncoderBlock(256, 512, 2)\n",
    "        self.last_layer = nn.Conv2d(512, 1, kernel_size=4, stride=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.discriminator_block_1(inputs)\n",
    "        output = self.discriminator_block_2(output)\n",
    "        output = self.discriminator_block_3(output)\n",
    "        output = self.discriminator_block_4(output)\n",
    "        output = torch.sigmoid(self.last_layer(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This is where the actual training occurs. We create instances of our networks and the data loader and then loop through a number of epochs and then train both the generator and the disciminator. On a CPU-based instance this will take about 8-10 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "b25e70a71a5d48548f60549b8dff2d10",
      "38c8da1b2c324c5a9658abb0bbf90ce6",
      "c349c2d356044831a60daba865323c81",
      "5ea5f5c8899741708b26626113671794",
      "cd0b16c4f188467e8038ad89a34c6568",
      "f4c1628e9cb241e1885131fa4e5e5cb1",
      "bf239b6d31a64b0fa47ec6f3c32072e6",
      "9ae6f1232207473eb6ae3ae4d775d766",
      "4634059c11064b448927b68a92e24462",
      "c15492a30b7c4deabfcc63d47bbc4d77",
      "8aeb772ded0a42748aa70a5284ba6801",
      "d4c658434d884309b274e9759a5d85b8",
      "7ee99db161c240a18670015c7d59d464",
      "6bcb28cf89ef4c9883619ceddbd3e061",
      "c2741226cc40492eaabccaf8775a0654",
      "15973d3fd2904fbb996cf841302a9099"
     ]
    },
    "colab_type": "code",
    "id": "BQt_TFUkmG0D",
    "outputId": "2bd637ec-bf5d-4f43-e5dd-08bdc66349cc"
   },
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator('G', training=True).to(device)\n",
    "discriminator = Discriminator('D', training=True).to(device)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "# Instantiate the data loader\n",
    "rgb_images = rgb_img_data(FACES_URL)\n",
    "lab_images = lab_img_data(rgb_images)\n",
    "trainset = tensor_img_data(lab_images)\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': True,\n",
    "          'drop_last': True,\n",
    "          'num_workers': 16}\n",
    "trainloader = DataLoader(trainset, **params)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr/10, betas=(b1, b2))\n",
    "\n",
    "\n",
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    for i, (l, ab) in tqdm(enumerate(trainloader), total=int(len(trainset) / batch_size)):\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(torch.Tensor(l.size(0), 1).fill_(1.0), requires_grad=False).to(device)\n",
    "        fake = Variable(torch.Tensor(l.size(0), 1).fill_(0.0), requires_grad=False).to(device)\n",
    "\n",
    "        lvar = Variable(l).to(device)\n",
    "        abvar = Variable(ab).to(device)\n",
    "        real_imgs = torch.cat([lvar,abvar],dim=1)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate a batch of images\n",
    "        abgen = generator(lvar)\n",
    "        gen_imgs = torch.cat([lvar.detach(),abgen],dim=1)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss_gan = adversarial_loss(discriminator(gen_imgs.detach()), valid)\n",
    "        g_loss = g_loss_gan + pixel_loss_weights * torch.mean((abvar-abgen)**2)\n",
    "\n",
    "        g_losses.append(g_loss)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_losses.append(d_loss)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        if i and (i + 1) % 50 == 0:\n",
    "            print(\"Epoch: %d, Iter: %d,\\tG loss: %.2f,\\tD loss: %.2f\" % (epoch, i + 1, g_loss.item(), d_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUyw0SfnBwFe"
   },
   "outputs": [],
   "source": [
    "# URL of a test image\n",
    "test_img_url = \"https://github.com/IBMDeveloperUK/Colourise-GAN-Workshop/raw/master/data/matt_rgb.jpg\"\n",
    "\n",
    "# Fetch the test image, resize it to correct size and convert it to numpy array\n",
    "r = requests.get(test_img_url)\n",
    "test_img = np.asarray(Image.open(io.BytesIO(r.content)).resize((64,64)))\n",
    "\n",
    "# Original RGB image\n",
    "plt.figure(figsize=[12,6])\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.axis('off')\n",
    "plt.title('Original RGB')\n",
    "plt.imshow(test_img)\n",
    "\n",
    "# Greyscale image\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.axis('off')\n",
    "plt.title('Original Greyscale')\n",
    "test_lab = rgb2lab(test_img)\n",
    "plt.imshow(test_lab[:,:,0], cmap='gray')\n",
    "\n",
    "# Generated image\n",
    "test_inf = test_lab[...,0].reshape(1,1,64,64)\n",
    "# We need to scale the L value of the LAB image to -1..1 for the neural network\n",
    "test_var = Variable(torch.Tensor(test_inf / 50 - 1)).to(device)\n",
    "\n",
    "# Generate the image by passing in the greyscale version\n",
    "test_res = generator(test_var)\n",
    "ab=test_res.cpu().detach().numpy()\n",
    "\n",
    "# We need to scale the AB values returned from the generator to be in the range -128..128\n",
    "ab[:,0,:,:] *= 128\n",
    "ab[:,1,:,:] *= 128\n",
    "\n",
    "# Convert from LAB back to RGB format.\n",
    "gen_lab = np.concatenate([test_inf,ab],axis=1).reshape(3,64,64)\n",
    "gen_rgb = lab2rgb(gen_lab.transpose(1,2,0))\n",
    "gen_rgb = np.clip(gen_rgb, 0, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.axis('off')\n",
    "plt.title('Generated RGB')\n",
    "plt.imshow(gen_rgb)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled17.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15973d3fd2904fbb996cf841302a9099": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38c8da1b2c324c5a9658abb0bbf90ce6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4634059c11064b448927b68a92e24462": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aeb772ded0a42748aa70a5284ba6801",
       "IPY_MODEL_d4c658434d884309b274e9759a5d85b8"
      ],
      "layout": "IPY_MODEL_c15492a30b7c4deabfcc63d47bbc4d77"
     }
    },
    "5ea5f5c8899741708b26626113671794": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ae6f1232207473eb6ae3ae4d775d766",
      "placeholder": "​",
      "style": "IPY_MODEL_bf239b6d31a64b0fa47ec6f3c32072e6",
      "value": " 0/10 [00:00&lt;?, ?it/s]"
     }
    },
    "6bcb28cf89ef4c9883619ceddbd3e061": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ee99db161c240a18670015c7d59d464": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8aeb772ded0a42748aa70a5284ba6801": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 14%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6bcb28cf89ef4c9883619ceddbd3e061",
      "max": 312,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ee99db161c240a18670015c7d59d464",
      "value": 44
     }
    },
    "9ae6f1232207473eb6ae3ae4d775d766": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b25e70a71a5d48548f60549b8dff2d10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c349c2d356044831a60daba865323c81",
       "IPY_MODEL_5ea5f5c8899741708b26626113671794"
      ],
      "layout": "IPY_MODEL_38c8da1b2c324c5a9658abb0bbf90ce6"
     }
    },
    "bf239b6d31a64b0fa47ec6f3c32072e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c15492a30b7c4deabfcc63d47bbc4d77": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2741226cc40492eaabccaf8775a0654": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c349c2d356044831a60daba865323c81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "  0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4c1628e9cb241e1885131fa4e5e5cb1",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd0b16c4f188467e8038ad89a34c6568",
      "value": 0
     }
    },
    "cd0b16c4f188467e8038ad89a34c6568": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d4c658434d884309b274e9759a5d85b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15973d3fd2904fbb996cf841302a9099",
      "placeholder": "​",
      "style": "IPY_MODEL_c2741226cc40492eaabccaf8775a0654",
      "value": " 44/312 [00:06&lt;00:33,  8.04it/s]"
     }
    },
    "f4c1628e9cb241e1885131fa4e5e5cb1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
